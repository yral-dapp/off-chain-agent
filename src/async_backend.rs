use std::{sync::Arc, time::Duration};

use anyhow::Context;
use candid::Principal;
use spacetimedb_sdk::{Status, Timestamp};
use tokio::sync::broadcast;
use yral_spacetime_bindings::autogenerated::backend::{self, add_notification, DbConnection};

use crate::consts::{STDB_ACCESS_TOKEN, STDB_URL};

pub type ReducerResult = Result<(), String>;

/// (input hash, reducer result)
type BusMessage = (Principal, backend::NotificationType, ReducerResult);

/// A wrapper around the [`dedup_index::DbConnection`] with an internal message bus that allows for async operations
#[derive(Clone)]
pub struct WrappedContext {
    pub conn: Arc<backend::DbConnection>,
    tx: broadcast::Sender<BusMessage>,
}

impl WrappedContext {
    pub fn new() -> anyhow::Result<Self> {
        let conn = DbConnection::builder()
            .with_uri(STDB_URL)
            .with_module_name("yral-backend")
            .with_token(Some(STDB_ACCESS_TOKEN.as_str()))
            .build()
            .context("Couldn't connect to the db")?;

        let conn = Arc::new(conn);
        let conn_for_ticking = conn.clone();

        tokio::spawn(async move {
            let Err(err) = conn_for_ticking.run_async().await else {
                return;
            };

            log::error!("connection to backend broke with an error: {err:#?}");
        });

        // this limit is for lagging mechanism of broadcast channel
        //
        // in our case, the receivers don't any slow work after receiving
        // messages, so we wont run into "slow receiver" problem.
        //
        // however, in case we end up with more than this limit number of
        // requests at the same time, the receiver would fail with error and
        // will most likely be retried by qstash
        let (tx, _) = broadcast::channel(65536);
        let tx_clone = tx.clone();

        conn.reducers
            .on_add_notification(move |event, principal, payload| {
                let res = match event.event.status {
                    Status::Committed => Ok(()),
                    Status::Failed(ref msg) => Err(msg.to_string()),
                    Status::OutOfEnergy => Err("Out of energy".into()),
                };

                // channel must be not be closed
                tx_clone
                    .send((
                        Principal::from_text(principal).unwrap(),
                        payload.clone(),
                        res,
                    ))
                    .unwrap();
            });

        Ok(Self { conn, tx })
    }

    /// Adds a video hash to dedup index
    ///
    /// Outer error is any error when sending request, for example, network error.
    /// The inner error is the result of the operation itself
    ///
    /// taking ownership as that will require caller to clone and we need the underlying rx to be cloned to add a listener
    pub async fn add_notification(
        &self,
        principal: Principal,
        payload: backend::NotificationType,
    ) -> anyhow::Result<ReducerResult> {
        let mut rx = self.tx.subscribe();
        self.conn
            .reducers
            .add_notification(principal.to_string(), payload.clone())
            .context("Couldn't send request to add")?;

        let res = loop {
            let (recv_principal, recv_payload, data) =
                tokio::time::timeout(Duration::from_secs(5), rx.recv())
                    .await
                    .context("timeout reached before receiving result")??;
            if recv_principal == principal && recv_payload == payload {
                break data;
            }
        };

        Ok(res)
    }
}
